# app.py
import streamlit as st
import ui                   # UIãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
import llm                  # LLMãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
import database             # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
import metrics              # è©•ä¾¡æŒ‡æ¨™ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
import data                 # ãƒ‡ãƒ¼ã‚¿ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
import torch
from transformers import pipeline
from config import MODEL_NAME
from huggingface_hub import HfFolder

## æ”¹å–„æ–¹é‡
# - æ™‚é–“é–“ã«åˆã‚ãšæ–­å¿µ ãƒ¢ãƒ‡ãƒ«ã‚’ `AutoModelForCausalLM` + `AutoTokenizer` ã«åˆ‡ã‚Šæ›¿ãˆã¦åˆ¶å¾¡æ€§ã¨é€Ÿåº¦ã‚’å‘ä¸Š
# - BLEUã‚¹ã‚³ã‚¢ã‚’ç”¨ã„ãŸè‡ªå‹•è©•ä¾¡æŒ‡æ¨™ã‚’å°å…¥ï¼Œå¿œç­”å“è³ªã‚’å¯è¦–åŒ–
# - ãƒãƒ£ãƒƒãƒˆç”»é¢ã§å±¥æ­´ã‚’å·¦ã‚µã‚¤ãƒ‰ãƒãƒ¼ã«è¡¨ç¤ºï¼Œä¼šè©±æ–‡è„ˆã®æŠŠæ¡ã‚’å¼·åŒ–

# ----- ãƒ¢ãƒ‡ãƒ«å -----
# MODEL_NAME = "distilgpt2"

# ----- BLEUã‚¹ã‚³ã‚¢ã‚’è©•ä¾¡æŒ‡æ¨™ã«å°å…¥ From -----
import nltk
from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction

def compute_bleu(reference, hypothesis):
    reference = [reference.split()]
    hypothesis = hypothesis.split()
    return sentence_bleu(reference, hypothesis, smoothing_function=SmoothingFunction().method1)

# ----- BLEUã‚¹ã‚³ã‚¢ã‚’è©•ä¾¡æŒ‡æ¨™ã«å°å…¥ To -----


# --- ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³è¨­å®š ---
st.set_page_config(page_title="Gemma Chatbot", layout="wide")

# --- åˆæœŸåŒ–å‡¦ç† ---
# NLTKãƒ‡ãƒ¼ã‚¿ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆåˆå›èµ·å‹•æ™‚ãªã©ï¼‰
metrics.initialize_nltk()

# ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®åˆæœŸåŒ–ï¼ˆãƒ†ãƒ¼ãƒ–ãƒ«ãŒå­˜åœ¨ã—ãªã„å ´åˆã€ä½œæˆï¼‰
database.init_db()

# ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãŒç©ºãªã‚‰ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’æŠ•å…¥
data.ensure_initial_data()

# LLMãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’åˆ©ç”¨ï¼‰
# ãƒ¢ãƒ‡ãƒ«ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã—ã¦å†åˆ©ç”¨
@st.cache_resource
def load_model():
    """LLMãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã™ã‚‹"""
    try:
        device = "cuda" if torch.cuda.is_available() else "cpu"
        st.info(f"Using device: {device}") # ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹ã‚’è¡¨ç¤º
        pipe = pipeline(
            "text-generation",
            model=MODEL_NAME,
            model_kwargs={"torch_dtype": torch.bfloat16},
            device=device
        )
        st.success(f"ãƒ¢ãƒ‡ãƒ« '{MODEL_NAME}' ã®èª­ã¿è¾¼ã¿ã«æˆåŠŸã—ã¾ã—ãŸã€‚")
        return pipe
    except Exception as e:
        st.error(f"ãƒ¢ãƒ‡ãƒ« '{MODEL_NAME}' ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        st.error("GPUãƒ¡ãƒ¢ãƒªä¸è¶³ã®å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚ä¸è¦ãªãƒ—ãƒ­ã‚»ã‚¹ã‚’çµ‚äº†ã™ã‚‹ã‹ã€ã‚ˆã‚Šå°ã•ã„ãƒ¢ãƒ‡ãƒ«ã®ä½¿ç”¨ã‚’æ¤œè¨ã—ã¦ãã ã•ã„ã€‚")
        return None
pipe = llm.load_model()

# ---- ãƒ¢ãƒ‡ãƒ«æ”¹å–„ From -----
# ãƒ¢ãƒ‡ãƒ«ã‚’ AutoModelForCausalLM + AutoTokenizer ã«å¤‰æ›´
# from transformers import AutoModelForCausalLM, AutoTokenizer
# import torch

# def load_model():
#     try:
#         device = "cuda" if torch.cuda.is_available() else "cpu"
#         tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
#         model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, torch_dtype=torch.float32).to(device)
#         st.success(f"âœ… ãƒ¢ãƒ‡ãƒ« '{MODEL_NAME}' ã‚’ {device} ä¸Šã§ãƒ­ãƒ¼ãƒ‰ã—ã¾ã—ãŸ")
#         return lambda prompt: tokenizer.decode(
#             model.generate(tokenizer(prompt, return_tensors="pt").input_ids.to(device), max_new_tokens=128)[0],
#             skip_special_tokens=True
#         )
#     except Exception as e:
#         st.error(f"ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—: {e}")
#         return None

# nltk.download("punkt") # NLTKåˆæœŸåŒ–

# pipe = load_model()

# ---- ãƒ¢ãƒ‡ãƒ«æ”¹å–„ To -----

# ----- ã‚»ãƒƒã‚·ãƒ§ãƒ³åˆæœŸåŒ–ï¼ˆå±¥æ­´ç®¡ç†ï¼‰ -----
if "history" not in st.session_state:
    st.session_state.history = []

# --- Streamlit ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ ---
# st.title("ğŸ¤– Gemma 2 Chatbot with Feedback")
# st.write("Gemmaãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ãŸãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã§ã™ã€‚å›ç­”ã«å¯¾ã—ã¦ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’è¡Œãˆã¾ã™ã€‚")
#  ----- å¤‰æ›´ From -----
st.title("ğŸ¤– Gemma Chatbot with BLEU Feedback")
st.write("Gemmaãƒ¢ãƒ‡ãƒ«ã‚’ç”¨ã„ãŸãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã§ã™ã€‚å„å¿œç­”ã«è‡ªå‹•è©•ä¾¡ï¼ˆBLEUã‚¹ã‚³ã‚¢ï¼‰ã‚’è¡¨ç¤ºã—ã¾ã™ã€‚")
#  ----- å¤‰æ›´ To -----

st.markdown("---")

# ----- å·¦ã‚µã‚¤ãƒ‰ãƒãƒ¼ï¼šå±¥æ­´è¡¨ç¤º From -----
st.sidebar.title("ğŸ’¬ ãƒãƒ£ãƒƒãƒˆå±¥æ­´")
for idx, (user, bot) in enumerate(st.session_state.history[::-1]):
    st.sidebar.markdown(f"**Q{len(st.session_state.history)-idx}**: {user}")
    st.sidebar.caption(f"â†’ {bot[:30]}...")

# ----- å·¦ã‚µã‚¤ãƒ‰ãƒãƒ¼ï¼šå±¥æ­´è¡¨ç¤º To -----


# # --- ã‚µã‚¤ãƒ‰ãƒãƒ¼ ---
# st.sidebar.title("ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³")
# # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã‚’ä½¿ç”¨ã—ã¦é¸æŠãƒšãƒ¼ã‚¸ã‚’ä¿æŒ
# if 'page' not in st.session_state:
#     st.session_state.page = "ãƒãƒ£ãƒƒãƒˆ" # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒšãƒ¼ã‚¸

# page = st.sidebar.radio(
#     "ãƒšãƒ¼ã‚¸é¸æŠ",
#     ["ãƒãƒ£ãƒƒãƒˆ", "å±¥æ­´é–²è¦§", "ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ç®¡ç†"],
#     key="page_selector",
#     index=["ãƒãƒ£ãƒƒãƒˆ", "å±¥æ­´é–²è¦§", "ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ç®¡ç†"].index(st.session_state.page), # ç¾åœ¨ã®ãƒšãƒ¼ã‚¸ã‚’é¸æŠçŠ¶æ…‹ã«ã™ã‚‹
#     on_change=lambda: setattr(st.session_state, 'page', st.session_state.page_selector) # é¸æŠå¤‰æ›´æ™‚ã«çŠ¶æ…‹ã‚’æ›´æ–°
# )


# # --- ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ ---
# if st.session_state.page == "ãƒãƒ£ãƒƒãƒˆ":
#     if pipe:
#         ui.display_chat_page(pipe)
#     else:
#         st.error("ãƒãƒ£ãƒƒãƒˆæ©Ÿèƒ½ã‚’åˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
# elif st.session_state.page == "å±¥æ­´é–²è¦§":

#     ui.display_history_page()
# elif st.session_state.page == "ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ç®¡ç†":
#     ui.display_data_page()


# ----- ãƒ¡ã‚¤ãƒ³ï¼šãƒãƒ£ãƒƒãƒˆå…¥åŠ›ãƒ»å¿œç­” From -----
user_input = st.text_input("ã‚ãªãŸã®è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")

if st.button("é€ä¿¡") and user_input:
    # ğŸ”§ å¿œç­”ã®ãƒ†ã‚­ã‚¹ãƒˆã ã‘ã‚’æŠ½å‡º
    response = pipe(user_input, max_new_tokens=1000)[0]['generated_text']

    # å¿œç­”è¡¨ç¤º
    st.markdown("### ğŸ¤– ãƒ¢ãƒ‡ãƒ«ã®å¿œç­”")
    st.success(response)

    # BLEUã‚¹ã‚³ã‚¢è¨ˆç®—
    bleu = compute_bleu(user_input, response)
    st.markdown(f"ğŸ§  BLEUã‚¹ã‚³ã‚¢: `{bleu:.2f}`")

    # å±¥æ­´ä¿å­˜
    st.session_state.history.append((user_input, response))

# ----- ãƒ¡ã‚¤ãƒ³ï¼šãƒãƒ£ãƒƒãƒˆå…¥åŠ›ãƒ»å¿œç­” To -----


# --- ãƒ•ãƒƒã‚¿ãƒ¼ãªã©ï¼ˆä»»æ„ï¼‰ ---
st.sidebar.markdown("---")
st.sidebar.info("å¤‰æ›´è€… : Yasuhiro Seino")






st.sidebar.markdown("---")
st.sidebar.info("é–‹ç™ºè€…: [Your Name]")